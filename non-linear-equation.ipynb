{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c71a4f",
   "metadata": {},
   "source": [
    "# Root Finding Methods for Equations of the Form $ f(x) = 0 $\n",
    "\n",
    "This repository contains an implementation and explanation of iterative methods used to approximate the roots of equations of the form:\n",
    "\n",
    "$$\n",
    "f(x) = 0\n",
    "$$\n",
    "\n",
    "## Overview\n",
    "\n",
    "In scientific computations, finding the roots of equations is a common problem. An **approximate solution** $ x^* $ to the equation satisfies $ f(x^*) \\approx 0 $. This project focuses on iterative methods to solve such equations.\n",
    "\n",
    "### Assumptions:\n",
    "- $ f(x) $ is a continuously differentiable real-valued function.\n",
    "- The equation $ f(x) = 0 $ has only isolated roots, meaning each root has a neighborhood that does not contain other roots.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "The process of approximating isolated real roots consists of two steps:\n",
    "\n",
    "1. **Initial Guess**: Establish the smallest interval $[a, b]$ containing one and only one root of the equation. Choose $ x_0 \\in [a, b] $ as an initial approximation to the root.\n",
    "2. **Improving Accuracy**: If $ x_0 $ is not sufficiently accurate, use an iterative method to refine it.\n",
    "\n",
    "### Iterative Process\n",
    "A general iterative method can be written as:\n",
    "$$\n",
    "x_{n+1} = T(x_n), \\quad n = 0, 1, \\dots\n",
    "$$\n",
    "where $ T $ is a real-valued function called the iteration function. The sequence $ \\{x_n\\} $ generated by this process is expected to converge to the root of $ f(x) = 0 $.\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence Definition\n",
    "\n",
    "A sequence $ \\{x_n\\} $ is said to converge with order $ p \\geq 1 $ to a point $ x^* $ if:\n",
    "$$\n",
    "|x_{n+1} - x^*| \\leq c |x_n - x^*|^p, \\quad n \\geq 0\n",
    "$$\n",
    "for some constant $ c > 0 $.\n",
    "\n",
    "- **Linear Convergence**: $ p = 1 $\n",
    "- **Quadratic Convergence**: $ p = 2 $\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Fixed-Point Iteration Method\n",
    "\n",
    "The fixed-point iteration method rewrites the equation $ f(x) = 0 $ in the form:\n",
    "$$\n",
    "x = g(x)\n",
    "$$\n",
    "where any solution of $ x = g(x) $ (a fixed point of $ g(x) $) is also a solution of $ f(x) = 0 $.\n",
    "\n",
    "### Steps:\n",
    "1. Choose an initial guess $ x_0 $.\n",
    "2. Define the iteration method:\n",
    "   $$\n",
    "   x_{n+1} = g(x_n), \\quad n = 0, 1, \\dots\n",
    "   $$\n",
    "\n",
    "### Properties of a Good Iteration Function $ g(x) $:\n",
    "1. Successive approximations $ x_n $ can be calculated from $ x_0 $.\n",
    "2. The sequence $ \\{x_n\\} $ converges to a point $ \\xi $.\n",
    "3. The limit $ \\xi $ is a fixed point of $ g(x) $, i.e., $ \\xi = g(\\xi) $.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing $ g(x) $\n",
    "\n",
    "To ensure convergence, $ g(x) $ must satisfy the following assumptions:\n",
    "\n",
    "1. **Domain Preservation**: $ a \\leq g(x) \\leq b $ for all $ a \\leq x \\leq b $.\n",
    "2. **Continuity**: $ g(x) $ is continuous.\n",
    "3. **Contraction Mapping**: $ g'(x) $ exists on $ [a, b] $, and there exists a constant $ 0 < K < 1 $ such that:\n",
    "   $$\n",
    "   |g'(x)| \\leq K, \\quad x \\in [a, b].\n",
    "   $$\n",
    "\n",
    "If these conditions are satisfied, the fixed-point iteration method guarantees:\n",
    "- A unique solution $ x^* $ in $ [a, b] $.\n",
    "- Convergence of the sequence $ \\{x_n\\} $ to $ x^* $ for any $ x_0 \\in [a, b] $.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Fixed-Point Iteration\n",
    "\n",
    "Consider the equation:\n",
    "$$\n",
    "\\sin(x) + x^2 - 1 = 0\n",
    "$$\n",
    "\n",
    "### Possible Iteration Functions:\n",
    "1. $ g_1(x) = \\sin^{-1}(1 - x^2) $\n",
    "2. $ g_2(x) = \\sqrt{1 - \\sin(x)} $\n",
    "3. $ g_3(x) = \\sqrt{1 - \\sin(x)} $\n",
    "\n",
    "#### Observations:\n",
    "- $ g_1(x) $ leads to divergence because $ |g_1'(x)| > 1 $.\n",
    "- $ g_2(x) $ violates domain preservation and is unsuitable.\n",
    "- $ g_3(x) $ satisfies all assumptions and converges when $ x_0 = 0.8 $.\n",
    "\n",
    "---\n",
    "\n",
    "## Stopping Criteria\n",
    "\n",
    "Iterative methods can be stopped based on one of the following conditions:\n",
    "\n",
    "1. **Condition 1**: Check the inequality:\n",
    "   $$\n",
    "   |x_n - x_{n-k}| < \\epsilon\n",
    "   $$\n",
    "   for some small $ \\epsilon > 0 $ and fixed $ k $.\n",
    "\n",
    "2. **Condition 2**: Check the residual error:\n",
    "   $$\n",
    "   |f(x_n)| < \\epsilon\n",
    "   $$\n",
    "\n",
    "These criteria ensure that the iterative process terminates when the desired accuracy is achieved.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project explores iterative methods for solving nonlinear equations of the form $ f(x) = 0 $. The fixed-point iteration method is discussed in detail, along with its theoretical foundations, practical implementation, and stopping criteria. By carefully choosing the iteration function $ g(x) $, we can achieve efficient and accurate solutions to a wide range of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4efd7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Bisection Method\n",
    "\n",
    "The **Bisection Method** is a simple and robust numerical technique for finding roots of equations of the form:\n",
    "\n",
    "$$\n",
    "f(x) = 0\n",
    "$$\n",
    "\n",
    "This method relies on repeatedly halving an interval and narrowing down the location of the root based on the sign changes of $ f(x) $. It guarantees convergence to a root under certain conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. The function $ f(x) $ is **continuous** on the interval $[a, b]$.\n",
    "2. $ f(a)f(b) < 0 $, meaning $ f(a) $ and $ f(b) $ have opposite signs.\n",
    "3. There is only one root in the interval $[a, b]$.\n",
    "\n",
    "Using the **Intermediate Value Theorem**, we can conclude that there exists at least one root $ x^* \\in [a, b] $.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "### Step-by-Step Procedure:\n",
    "1. **Initialization**: Start with the initial interval $[a_0, b_0]$, where $ f(a_0)f(b_0) < 0 $. Set $ n = 0 $.\n",
    "2. **Midpoint Calculation**: Compute the midpoint of the interval:\n",
    "   $$\n",
    "   c_{n+1} = \\frac{a_n + b_n}{2}.\n",
    "   $$\n",
    "3. **Root Check**:\n",
    "   - If $ f(a_n)f(c_{n+1}) = 0 $, then $ x^* = c_{n+1} $ is the root.\n",
    "   - If $ f(a_n)f(c_{n+1}) < 0 $, the root lies in $[a_n, c_{n+1}]$. Update:\n",
    "     $$\n",
    "     a_{n+1} = a_n, \\quad b_{n+1} = c_{n+1}.\n",
    "     $$\n",
    "   - If $ f(a_n)f(c_{n+1}) > 0 $, the root lies in $[c_{n+1}, b_n]$. Update:\n",
    "     $$\n",
    "     a_{n+1} = c_{n+1}, \\quad b_{n+1} = b_n.\n",
    "     $$\n",
    "4. **Stopping Criterion**:\n",
    "   - If the length of the interval $ b_{n+1} - a_{n+1} $ is less than a prescribed tolerance $ \\epsilon $, stop the iteration and take the midpoint of the interval as the approximate root:\n",
    "     $$\n",
    "     x^* = \\frac{b_{n+1} + a_{n+1}}{2}.\n",
    "     $$\n",
    "   - Otherwise, repeat from Step 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence and Error Analysis\n",
    "\n",
    "### Theorem 4.7 (Convergence and Error of Bisection Method)\n",
    "\n",
    "Let $[a_0, b_0] = [a, b]$ be the initial interval, with $ f(a)f(b) < 0 $. Define the approximate root as:\n",
    "$$\n",
    "x_n = \\frac{b_n + a_n}{2}.\n",
    "$$\n",
    "\n",
    "Then:\n",
    "1. There exists a root $ x^* \\in [a, b] $ such that:\n",
    "   $$\n",
    "   |x_n - x^*| \\leq \\left(\\frac{1}{2}\\right)^n (b - a).\n",
    "   $$\n",
    "2. To achieve an accuracy of $ |x_n - x^*| \\leq \\epsilon $, it suffices to perform:\n",
    "   $$\n",
    "   n \\geq \\frac{\\log(b - a) - \\log(\\epsilon)}{\\log(2)}.\n",
    "   $$\n",
    "\n",
    "### Proof:\n",
    "1. The interval length halves at each iteration:\n",
    "   $$\n",
    "   b_n - a_n = \\frac{1}{2}(b_{n-1} - a_{n-1}),\n",
    "   $$\n",
    "   leading to:\n",
    "   $$\n",
    "   b_n - a_n = \\left(\\frac{1}{2}\\right)^n (b_0 - a_0).\n",
    "   $$\n",
    "2. The error bound is derived as:\n",
    "   $$\n",
    "   |x_n - x^*| \\leq \\frac{1}{2}(b_n - a_n) = \\left(\\frac{1}{2}\\right)^n (b - a).\n",
    "   $$\n",
    "3. To ensure $ |x_n - x^*| \\leq \\epsilon $, solve:\n",
    "   $$\n",
    "   \\left(\\frac{1}{2}\\right)^n (b - a) \\leq \\epsilon,\n",
    "   $$\n",
    "   which gives:\n",
    "   $$\n",
    "   n \\geq \\frac{\\log(b - a) - \\log(\\epsilon)}{\\log(2)}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Bisection Method\n",
    "\n",
    "Consider the equation:\n",
    "$$\n",
    "\\sin(x) + x^2 - 1 = 0,\n",
    "$$\n",
    "with the initial interval $[a_0, b_0] = [0, 1]$.\n",
    "\n",
    "### Given Data:\n",
    "- Tolerance: $ \\epsilon = 0.125 $.\n",
    "- Function: $ f(x) = \\sin(x) + x^2 - 1 $.\n",
    "\n",
    "### Iterations:\n",
    "1. **Iteration 1**:\n",
    "   - $ a_0 = 0, b_0 = 1 $.\n",
    "   - Midpoint: $ c_1 = \\frac{0 + 1}{2} = 0.5 $.\n",
    "   - $ f(c_1) = \\sin(0.5) + 0.5^2 - 1 = -0.27 < 0 $.\n",
    "   - Update: $ a_1 = 0.5, b_1 = 1 $.\n",
    "\n",
    "2. **Iteration 2**:\n",
    "   - $ a_1 = 0.5, b_1 = 1 $.\n",
    "   - Midpoint: $ c_2 = \\frac{0.5 + 1}{2} = 0.75 $.\n",
    "   - $ f(c_2) = \\sin(0.75) + 0.75^2 - 1 = 0.24 > 0 $.\n",
    "   - Update: $ a_2 = 0.5, b_2 = 0.75 $.\n",
    "\n",
    "3. **Iteration 3**:\n",
    "   - $ a_2 = 0.5, b_2 = 0.75 $.\n",
    "   - Midpoint: $ c_3 = \\frac{0.5 + 0.75}{2} = 0.625 $.\n",
    "   - $ f(c_3) = \\sin(0.625) + 0.625^2 - 1 = -0.024 < 0 $.\n",
    "   - Update: $ a_3 = 0.625, b_3 = 0.75 $.\n",
    "\n",
    "### Stopping Criterion:\n",
    "- Interval length: $ |b_3 - a_3| = 0.75 - 0.625 = 0.125 $.\n",
    "- Since $ |b_3 - a_3| \\leq \\epsilon $, stop the iteration.\n",
    "\n",
    "### Approximate Root:\n",
    "- Take the midpoint of the final interval:\n",
    "  $$\n",
    "  x^* = \\frac{0.625 + 0.75}{2} = 0.6875.\n",
    "  $$\n",
    "\n",
    "### True Root:\n",
    "- The true root is approximately $ x^* \\approx 0.636733 $.\n",
    "- Absolute error: $ |0.6875 - 0.636733| = 0.05 $.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The **Bisection Method** is a reliable and straightforward approach for finding roots of continuous functions. While it converges linearly, its simplicity and guaranteed convergence make it a valuable tool in numerical analysis. By carefully selecting the initial interval and stopping criterion, this method ensures accurate approximations to the desired root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afafd93",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Secant Method\n",
    "\n",
    "The **Secant Method** is one of the most efficient numerical techniques for finding roots of equations. It is closely related to the **Regula-Falsi Method**, which itself is an improvement over the Bisection Method. While the Bisection Method guarantees convergence, it can be slow in cases where the root is closer to one boundary of the interval. The Regula-Falsi and Secant Methods address this issue by using a weighted average (or secant line) to better approximate the root.\n",
    "\n",
    "---\n",
    "\n",
    "## Regula-Falsi Method\n",
    "\n",
    "The **Regula-Falsi Method** works by iteratively narrowing down the interval containing the root using a weighted average of the function values at the endpoints. This weighted average corresponds to the point where the secant line joining the points $(a, f(a))$ and $(b, f(b))$ intersects the x-axis.\n",
    "\n",
    "### Algorithm:\n",
    "1. **Initialization**: Start with an initial interval $[a_0, b_0]$, where $f(a_0)f(b_0) < 0$. Set $n = 0$.\n",
    "2. **Weighted Average Calculation**: Compute the weighted average:\n",
    "   $$\n",
    "   w_{n+1} = \\frac{f(b_n)a_n - f(a_n)b_n}{f(b_n) - f(a_n)}.\n",
    "   $$\n",
    "3. **Root Check**:\n",
    "   - If $f(a_n)f(w_{n+1}) = 0$, then $x^* = w_{n+1}$ is the root.\n",
    "   - If $f(a_n)f(w_{n+1}) < 0$, update:\n",
    "     $$\n",
    "     a_{n+1} = a_n, \\quad b_{n+1} = w_{n+1}.\n",
    "     $$\n",
    "   - If $f(a_n)f(w_{n+1}) > 0$, update:\n",
    "     $$\n",
    "     a_{n+1} = w_{n+1}, \\quad b_{n+1} = b_n.\n",
    "   $$\n",
    "4. **Stopping Criterion**:\n",
    "   - Check if $|f(w_{n+1})| < \\epsilon$ for some pre-assigned tolerance $\\epsilon$. If true, take $w_{n+1}$ as the approximate root.\n",
    "   - Otherwise, repeat from Step 2.\n",
    "\n",
    "### Derivation of Weighted Average:\n",
    "The weighted average $w$ is derived from the intersection of the secant line joining $(a, f(a))$ and $(b, f(b))$ with the x-axis. The equation of the secant line is:\n",
    "$$\n",
    "s(x) = \\frac{f(a)(x - b) + f(b)(a - x)}{a - b}.\n",
    "$$\n",
    "Equating $s(x)$ to zero gives:\n",
    "$$\n",
    "w = \\frac{f(b)a - f(a)b}{f(b) - f(a)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Secant Method\n",
    "\n",
    "The **Secant Method** improves upon the Regula-Falsi Method by removing the requirement that the root lies within the interval $[a, b]$. Instead, it uses two initial guesses $x_0$ and $x_1$ (not necessarily on opposite sides of the root) and iteratively refines the approximation.\n",
    "\n",
    "### Iteration Formula:\n",
    "Given initial guesses $x_0$ and $x_1$, the iteration formula for the Secant Method is:\n",
    "$$\n",
    "x_{n+1} = \\frac{f(x_n)x_{n-1} - f(x_{n-1})x_n}{f(x_n) - f(x_{n-1})}.\n",
    "$$\n",
    "This can also be written as:\n",
    "$$\n",
    "x_{n+1} = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Secant Method\n",
    "\n",
    "Consider the equation:\n",
    "$$\n",
    "\\sin(x) + x^2 - 1 = 0,\n",
    "$$\n",
    "with initial guesses $x_0 = 0$ and $x_1 = 1$.\n",
    "\n",
    "### Iterations:\n",
    "1. **Iteration 2**:\n",
    "   - $x_0 = 0, x_1 = 1$.\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_2 = 1 - \\frac{\\sin(1) + 1^2 - 1}{\\sin(1) + 1^2 - (\\sin(0) + 0^2 - 1)} = 0.543044.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_2 - x^*| = |0.543044 - 0.636733| = 0.093689$.\n",
    "\n",
    "2. **Iteration 3**:\n",
    "   - $x_1 = 1, x_2 = 0.543044$.\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_3 = 0.543044 - \\frac{\\sin(0.543044) + (0.543044)^2 - 1}{\\sin(0.543044) + (0.543044)^2 - (\\sin(1) + 1^2 - 1)} = 0.626623.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_3 - x^*| = |0.626623 - 0.636733| = 0.010110$.\n",
    "\n",
    "3. **Iteration 4**:\n",
    "   - $x_2 = 0.543044, x_3 = 0.626623$.\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_4 = 0.626623 - \\frac{\\sin(0.626623) + (0.626623)^2 - 1}{\\sin(0.626623) + (0.626623)^2 - (\\sin(0.543044) + (0.543044)^2 - 1)} = 0.637072.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_4 - x^*| = |0.637072 - 0.636733| = 0.000339$.\n",
    "\n",
    "4. **Iteration 5**:\n",
    "   - $x_3 = 0.626623, x_4 = 0.637072$.\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_5 = 0.637072 - \\frac{\\sin(0.637072) + (0.637072)^2 - 1}{\\sin(0.637072) + (0.637072)^2 - (\\sin(0.626623) + (0.626623)^2 - 1)} = 0.636732.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_5 - x^*| = |0.636732 - 0.636733| = 0.000001$.\n",
    "\n",
    "### Convergence:\n",
    "The exact solution is $x^* \\approx 0.636733$. The Secant Method converges quickly, achieving high accuracy in just a few iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## Order of Convergence\n",
    "\n",
    "The **order of convergence** of the Secant Method is given by:\n",
    "$$\n",
    "r = \\frac{\\sqrt{5} + 1}{2} \\approx 1.62.\n",
    "$$\n",
    "This means the error decreases approximately as:\n",
    "$$\n",
    "|x_{n+1} - x^*| \\leq C |x_n - x^*|^r,\n",
    "$$\n",
    "where $C$ is a constant depending on the function.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison with Other Methods\n",
    "\n",
    "- **Bisection Method**: Guarantees convergence but is slower.\n",
    "- **Fixed-Point Iteration**: Simple but may converge slowly or diverge depending on the choice of $g(x)$.\n",
    "- **Secant Method**: Faster than both Bisection and Fixed-Point Iteration, with superlinear convergence ($r \\approx 1.62$).\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The **Secant Method** is a powerful numerical technique for solving nonlinear equations. By leveraging the concept of secant lines, it achieves faster convergence compared to simpler methods like Bisection or Fixed-Point Iteration. Its efficiency makes it a popular choice in practical applications, especially when derivative information is unavailable or costly to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ba16f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Newton-Raphson Method\n",
    "\n",
    "The **Newton-Raphson Method** is a powerful numerical technique for finding roots of equations. It uses the derivative of the function to approximate the root iteratively, making it faster than methods like Bisection or Secant.\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration Formula\n",
    "\n",
    "If $ f(x) $ is differentiable, the Newton-Raphson iteration formula is derived by replacing the slope of the secant line with the slope of the tangent at $ x_n $:\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.\n",
    "$$\n",
    "\n",
    "This formula can also be interpreted as finding the fixed point of the function:\n",
    "$$\n",
    "g(x) = x - \\frac{f(x)}{f'(x)}.\n",
    "$$\n",
    "\n",
    "### Analytical Derivation\n",
    "Using Taylor's expansion of $ f(x) $ around $ x_0 $:\n",
    "$$\n",
    "f(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{(x - x_0)^2}{2!} f''(\\xi),\n",
    "$$\n",
    "where $ \\xi $ lies between $ x_0 $ and $ x $. Substituting $ x = x^* $ (the root), we get:\n",
    "$$\n",
    "0 = f(x_0) + f'(x_0)(x^* - x_0) + \\frac{(x^* - x_0)^2}{2!} f''(\\xi).\n",
    "$$\n",
    "Neglecting the higher-order term (valid when $ x_0 $ is close to $ x^* $):\n",
    "$$\n",
    "f(x_0) + f'(x_0)(x^* - x_0) \\approx 0.\n",
    "$$\n",
    "Solving for $ x^* $ gives:\n",
    "$$\n",
    "x^* \\approx x_0 - \\frac{f(x_0)}{f'(x_0)}.\n",
    "$$\n",
    "Replacing $ x_0 $ with $ x_n $ yields the general formula:\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Newton-Raphson Method\n",
    "\n",
    "Consider the equation:\n",
    "$$\n",
    "\\sin(x) + x^2 - 1 = 0,\n",
    "$$\n",
    "with initial guess $ x_0 = 1 $.\n",
    "\n",
    "### Iterations:\n",
    "1. **Iteration 1**:\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_1 = 1 - \\frac{\\sin(1) + 1^2 - 1}{\\cos(1) + 2 \\cdot 1} = 0.668752.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_1 - x^*| = |0.668752 - 0.636733| = 0.032019$.\n",
    "\n",
    "2. **Iteration 2**:\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_2 = 0.668752 - \\frac{\\sin(0.668752) + (0.668752)^2 - 1}{\\cos(0.668752) + 2 \\cdot 0.668752} = 0.637068.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_2 - x^*| = |0.637068 - 0.636733| = 0.000335$.\n",
    "\n",
    "3. **Iteration 3**:\n",
    "   - Compute:\n",
    "     $$\n",
    "     x_3 = 0.637068 - \\frac{\\sin(0.637068) + (0.637068)^2 - 1}{\\cos(0.637068) + 2 \\cdot 0.637068} = 0.636733.\n",
    "     $$\n",
    "   - Absolute error: $\\epsilon = |x_3 - x^*| = |0.636733 - 0.636733| = 0.000000$.\n",
    "\n",
    "### Convergence:\n",
    "The exact solution is $ x^* \\approx 0.636733 $. The Newton-Raphson Method converges very quickly, achieving high accuracy in just a few iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence Analysis\n",
    "\n",
    "### Theorem 4.14\n",
    "Assume $ f \\in C^2[a, b] $ and there exists a root $ x^* \\in [a, b] $ such that $ f(x^*) = 0 $. If $ f'(x^*) \\neq 0 $, then there exists a $ \\delta > 0 $ such that the sequence $ \\{x_n\\} $ defined by the Newton-Raphson iteration converges to $ x^* $ for any initial approximation $ x_0 \\in [x^* - \\delta, x^* + \\delta] $.\n",
    "\n",
    "Furthermore, the order of convergence is quadratic, given by:\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{|x_{n+1} - x^*|}{|x_n - x^*|^2} = \\frac{|f''(x^*)|}{2|f'(x^*)|}.\n",
    "$$\n",
    "\n",
    "### Proof Sketch:\n",
    "The proof involves analyzing the fixed-point iteration function $ g(x) = x - \\frac{f(x)}{f'(x)} $. By Taylor's theorem and properties of $ g(x) $, it can be shown that $ g'(x^*) = 0 $, ensuring quadratic convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Pitfalls of the Newton-Raphson Method\n",
    "\n",
    "While the Newton-Raphson Method is highly efficient, it has some pitfalls:\n",
    "\n",
    "1. **Zero Derivative**: If $ f'(x_n) = 0 $ for some $ n $, the method fails because division by zero occurs.\n",
    "2. **No Real Root**: If $ f(x) $ has no real root, the method may oscillate indefinitely. For example, $ f(x) = x^2 - 4x + 5 $ has no real roots.\n",
    "3. **Wrong Root Selection**: If $ f(x) = 0 $ has multiple roots and the initial guess is far from the desired root, the method may converge to another root. For instance, starting near $ x = 3 $ for $ f(x) = \\cos(x) $ may lead to convergence to $ 3\\pi/2 $ instead of $ \\pi/2 $.\n",
    "4. **Divergence on Unbounded Intervals**: If $ f(x) $ is monotonic and decreases rapidly on an unbounded interval, the method may diverge. For example, $ f(x) = xe^{-x} $ with $ x_0 = 2 $ leads to divergence.\n",
    "5. **Cycles**: The method can get stuck in cycles. For $ f(x) = x^3 - x - 3 $ with $ x_0 = 0 $, the sequence alternates without converging.\n",
    "6. **Oscillatory Divergence**: When $ |g'(x)| \\geq 1 $ near the root, the method may exhibit oscillatory divergence. For $ f(x) = \\tan^{-1}(x) $, starting with $ x_0 = 1.45 $ causes divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Quadratic Convergence\n",
    "\n",
    "Consider $ f(x) = x^3 - 3x + 2 $ with the root $ x^* = 2 $. Starting with $ x_0 = 2.4 $, the iteration formula is:\n",
    "$$\n",
    "x_{k+1} = g(x_k) = \\frac{2x_k^3 - 2}{3x_k^2 - 3}.\n",
    "$$\n",
    "Verify that:\n",
    "$$\n",
    "\\frac{|x^* - x_{n+1}|}{|x^* - x_n|^2} \\approx \\frac{2}{3}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The **Newton-Raphson Method** is a highly efficient numerical technique for solving nonlinear equations. Its quadratic convergence makes it superior to methods like Bisection or Fixed-Point Iteration. However, care must be taken to avoid pitfalls such as zero derivatives, divergence, or convergence to unintended roots. Proper selection of the initial guess and understanding the behavior of $ f(x) $ are crucial for successful application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982241a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. System of Nonlinear Equations\n",
    "\n",
    "This section discusses the theory and numerical methods for solving systems of nonlinear equations. We focus on two equations for simplicity, but the approach can be generalized to any finite number of equations.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "Consider a system of two nonlinear equations:\n",
    "$$\n",
    "f_1(x_1, x_2) = 0, \\quad f_2(x_1, x_2) = 0.\n",
    "$$\n",
    "In vector notation, this can be written as:\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = 0, \\quad \\mathbf{x} = (x_1, x_2)^T, \\quad \\mathbf{f}(\\mathbf{x}) = (f_1(x_1, x_2), f_2(x_1, x_2))^T.\n",
    "$$\n",
    "We assume that the system admits an isolated root $\\mathbf{x}^* = (x_1^*, x_2^*)^T$.\n",
    "\n",
    "---\n",
    "\n",
    "## Fixed-Point Iteration Method\n",
    "\n",
    "To solve the system iteratively, we define:\n",
    "$$\n",
    "x_{1, n+1} = g_1(x_{1,n}, x_{2,n}), \\quad x_{2, n+1} = g_2(x_{1,n}, x_{2,n}),\n",
    "$$\n",
    "where $g_1$ and $g_2$ are iterative functions. In vector form:\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\mathbf{g}(\\mathbf{x}_n), \\quad n = 0, 1, \\dots,\n",
    "$$\n",
    "with $\\mathbf{x}_n = (x_{1,n}, x_{2,n})^T$ and $\\mathbf{g}(\\mathbf{x}) = (g_1(x_1, x_2), g_2(x_1, x_2))^T$.\n",
    "\n",
    "The convergence of this method depends on the choice of $\\mathbf{g}$.\n",
    "\n",
    "### Convergence Analysis\n",
    "\n",
    "Using Taylor's expansion, we analyze the error propagation for the fixed-point iteration. Let $\\mathbf{x}^* = (x_1^*, x_2^*)$ be the root of the system. Then:\n",
    "$$\n",
    "x_1^* = g_1(x_1^*, x_2^*), \\quad x_2^* = g_2(x_1^*, x_2^*).\n",
    "$$\n",
    "Expanding $g_i(x_1, x_2)$ around $(x_1^*, x_2^*)$, we get:\n",
    "$$\n",
    "g_i(x_1^*, x_2^*) = g_i(x_{1,n}, x_{2,n}) + \\frac{\\partial g_i}{\\partial x_1}(\\xi_{1,n})(x_1^* - x_{1,n}) + \\frac{\\partial g_i}{\\partial x_2}(\\xi_{2,n})(x_2^* - x_{2,n}),\n",
    "$$\n",
    "where $(\\xi_{1,n}, \\xi_{2,n})$ lies on the line segment joining $\\mathbf{x}^*$ and $\\mathbf{x}_n$.\n",
    "\n",
    "Subtracting the iterative formula from the above equation, we obtain:\n",
    "$$\n",
    "x_i^* - x_{i,n+1} = \\frac{\\partial g_i}{\\partial x_1}(\\xi_{1,n})(x_1^* - x_{1,n}) + \\frac{\\partial g_i}{\\partial x_2}(\\xi_{2,n})(x_2^* - x_{2,n}), \\quad i = 1, 2.\n",
    "$$\n",
    "In matrix form:\n",
    "$$\n",
    "\\mathbf{x}^* - \\mathbf{x}_{n+1} = G_n (\\mathbf{x}^* - \\mathbf{x}_n),\n",
    "$$\n",
    "where $G_n$ is the Jacobian matrix of $\\mathbf{g}$ evaluated at intermediate points.\n",
    "\n",
    "### Convergence Theorem\n",
    "\n",
    "**Theorem 4.16**: Let $D$ be a closed, bounded, and convex set in the plane. Assume:\n",
    "1. $\\mathbf{g}(D) \\subset D$,\n",
    "2. $\\lambda = \\max_{\\mathbf{x} \\in D} \\|G(\\mathbf{x})\\|_\\infty < 1$.\n",
    "\n",
    "Then:\n",
    "1. $\\mathbf{x} = \\mathbf{g}(\\mathbf{x})$ has a unique solution $\\mathbf{x}^* \\in D$,\n",
    "2. For any initial point $\\mathbf{x}_0 \\in D$, the iteration $\\mathbf{x}_{n+1} = \\mathbf{g}(\\mathbf{x}_n)$ converges to $\\mathbf{x}^*$,\n",
    "3. The error satisfies:\n",
    "   $$\n",
    "   \\|\\mathbf{x}^* - \\mathbf{x}_{n+1}\\|_\\infty \\leq (\\|G(\\mathbf{x}^*)\\|_\\infty + \\epsilon_n) \\|\\mathbf{x}^* - \\mathbf{x}_n\\|_\\infty,\n",
    "   $$\n",
    "   where $\\epsilon_n \\to 0$ as $n \\to \\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "## Newton's Method for Systems of Equations\n",
    "\n",
    "For faster convergence, we use **Newton's Method**, which is derived by approximating the system using a linearization. Let $A$ be a constant nonsingular $2 \\times 2$ matrix. Rewrite the system as:\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{x} + A \\mathbf{f}(\\mathbf{x}) =: \\mathbf{g}(\\mathbf{x}).\n",
    "$$\n",
    "The Jacobian matrix of $\\mathbf{g}(\\mathbf{x})$ is:\n",
    "$$\n",
    "G(\\mathbf{x}) = I + A F(\\mathbf{x}),\n",
    "$$\n",
    "where $F(\\mathbf{x})$ is the Jacobian matrix of $\\mathbf{f}(\\mathbf{x})$:\n",
    "$$\n",
    "F(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "To ensure convergence, choose $A$ such that $\\|G(\\mathbf{x})\\|_\\infty < 1$. Practically, we often choose $A = (F(\\mathbf{x}_n))^{-1}$, leading to the iteration:\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\mathbf{x}_n - (F(\\mathbf{x}_n))^{-1} \\mathbf{f}(\\mathbf{x}_n), \\quad n = 0, 1, \\dots.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Newton's Method\n",
    "\n",
    "Consider solving the system:\n",
    "$$\n",
    "f_1 = 3x_1^2 + 4x_2^2 - 1 = 0, \\quad f_2 = x_2^3 - 8x_1^3 - 1 = 0,\n",
    "$$\n",
    "with initial guess $\\mathbf{x}_0 = (0.5, 0.25)$.\n",
    "\n",
    "### Step 1: Compute the Jacobian Matrix\n",
    "The Jacobian matrix is:\n",
    "$$\n",
    "F(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "6x_1 & 8x_2 \\\\\n",
    "-24x_1^2 & 3x_2^2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "At $\\mathbf{x}_0 = (0.5, 0.25)$:\n",
    "$$\n",
    "F(\\mathbf{x}_0) =\n",
    "\\begin{bmatrix}\n",
    "6(0.5) & 8(0.25) \\\\\n",
    "-24(0.5)^2 & 3(0.25)^2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 & 2 \\\\\n",
    "-6 & 0.1875\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The inverse of $F(\\mathbf{x}_0)$ is:\n",
    "$$\n",
    "F^{-1}(\\mathbf{x}_0) =\n",
    "\\begin{bmatrix}\n",
    "0.0164 & 0.1749 \\\\\n",
    "0.5246 & 0.2623\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Step 2: Perform Iteration\n",
    "The Newton iteration is given by:\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\mathbf{x}_n - F^{-1}(\\mathbf{x}_n) \\mathbf{f}(\\mathbf{x}_n).\n",
    "$$\n",
    "\n",
    "For the first iteration:\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}_0) =\n",
    "\\begin{bmatrix}\n",
    "3(0.5)^2 + 4(0.25)^2 - 1 \\\\\n",
    "(0.25)^3 - 8(0.5)^3 - 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-0.0156\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Update:\n",
    "$$\n",
    "\\mathbf{x}_1 =\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.25\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "0.0164 & 0.1749 \\\\\n",
    "0.5246 & 0.2623\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-0.0156\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.4973 \\\\\n",
    "0.2541\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Repeat the process until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The **Fixed-Point Iteration Method** and **Newton's Method** are powerful tools for solving systems of nonlinear equations. While the former relies on careful selection of iterative functions, the latter offers rapid quadratic convergence by leveraging the Jacobian matrix. Practical implementation requires efficient computation of derivatives and matrix inverses, making Newton's Method particularly effective for well-behaved systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67977174",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Unconstrained Optimization\n",
    "\n",
    "Optimization involves finding the **maximum** or **minimum** of a continuous function $ f(x_1, x_2, \\dots, x_n) $. In this section, we focus on finding **local minima** of $ f(x) $.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Definitions\n",
    "\n",
    "- A point $ \\mathbf{x}^* $ is called a **strict local minimum** of $ f(x) $ if:\n",
    "  $$\n",
    "  f(\\mathbf{x}) > f(\\mathbf{x}^*) \\quad \\text{for all } \\mathbf{x} \\text{ in a small neighborhood of } \\mathbf{x}^*.\n",
    "  $$\n",
    "\n",
    "- A necessary condition for $ \\mathbf{x}^* $ to be a strict local minimum is that the gradient of $ f(x) $ vanishes at $ \\mathbf{x}^* $:\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x_i} = 0, \\quad i = 1, 2, \\dots, n.\n",
    "  $$\n",
    "\n",
    "In vector notation, this system can be written as:\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = 0,\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)^T.\n",
    "$$\n",
    "\n",
    "To solve this system, **Newton's Method** can be applied. The iteration formula is:\n",
    "$$\n",
    "\\mathbf{x}_{n+1} = \\mathbf{x}_n - H(\\mathbf{x}_n)^{-1} \\nabla f(\\mathbf{x}_n), \\quad n = 0, 1, 2, \\dots,\n",
    "$$\n",
    "where $ H(\\mathbf{x}) $ is the **Hessian matrix** of $ f $, given by:\n",
    "$$\n",
    "H(\\mathbf{x})_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}, \\quad 1 \\leq i, j \\leq n.\n",
    "$$\n",
    "\n",
    "If $ \\mathbf{x}^* $ is a strict local minimum, the Hessian $ H(\\mathbf{x}^*) $ is nonsingular, and $ H(\\mathbf{x}) $ remains nonsingular in a small neighborhood of $ \\mathbf{x}^* $.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Newton's Method for Optimization\n",
    "\n",
    "Consider the function:\n",
    "$$\n",
    "f(x_1, x_2) = x_1^3 + 4x_1x_2^2 + x_1 - x_2.\n",
    "$$\n",
    "\n",
    "### Step 1: Compute the Gradient\n",
    "The partial derivatives of $ f(x_1, x_2) $ are:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1} = 3x_1^2 + 4x_2^2 + 1, \\quad \\frac{\\partial f}{\\partial x_2} = 8x_1x_2 - 1.\n",
    "$$\n",
    "\n",
    "Setting the gradient to zero gives the system of equations:\n",
    "$$\n",
    "3x_1^2 + 4x_2^2 + 1 = 0, \\tag{1}\n",
    "$$\n",
    "$$\n",
    "8x_1x_2 - 1 = 0. \\tag{2}\n",
    "$$\n",
    "\n",
    "### Step 2: Compute the Hessian Matrix\n",
    "The Hessian matrix $ H(x_1, x_2) $ is:\n",
    "$$\n",
    "H(x_1, x_2) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6x_1 & 8x_2 \\\\\n",
    "8x_2 & 8x_1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Step 3: Compute the Inverse of the Hessian\n",
    "The determinant of $ H(x_1, x_2) $ is:\n",
    "$$\n",
    "\\det(H(x_1, x_2)) = (6x_1)(8x_1) - (8x_2)(8x_2) = 48x_1^2 - 64x_2^2.\n",
    "$$\n",
    "\n",
    "The inverse of $ H(x_1, x_2) $ is:\n",
    "$$\n",
    "H^{-1}(x_1, x_2) = \\frac{1}{48x_1^2 - 64x_2^2}\n",
    "\\begin{bmatrix}\n",
    "8x_1 & -8x_2 \\\\\n",
    "-8x_2 & 6x_1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Step 4: Newton's Iteration Formula\n",
    "Using Newton's method, the iteration formula becomes:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,n+1} \\\\\n",
    "x_{2,n+1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{1,n} \\\\\n",
    "x_{2,n}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\frac{1}{8(3x_{1,n}^2 - 4x_{2,n}^2)}\n",
    "\\begin{bmatrix}\n",
    "4x_{1,n} & -4x_{2,n} \\\\\n",
    "-4x_{2,n} & 3x_{1,n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "3x_{1,n}^2 + 4x_{2,n}^2 + 1 \\\\\n",
    "8x_{1,n}x_{2,n} - 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Step 5: Perform Iterations\n",
    "Given an initial guess $ \\mathbf{x}_0 = (x_{1,0}, x_{2,0})^T $, the above formula can be used iteratively to compute successive approximations $ \\mathbf{x}_1, \\mathbf{x}_2, \\dots $ until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Unconstrained optimization involves finding points where the gradient of a function vanishes. Newton's Method provides a powerful iterative approach for solving such problems by leveraging the Hessian matrix. While computationally intensive due to the need for second derivatives, it offers rapid quadratic convergence when properly applied. Practical implementation requires careful handling of the Hessian matrix and its inverse, especially near critical points."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
